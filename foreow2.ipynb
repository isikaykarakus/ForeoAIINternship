{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRbAzc1IXrH8G5cM7ZYiyM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isikaykarakus/Foreo_AI_Internship/blob/main/foreow2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifsv3KXISg0n"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Multilingual PoC: EN/ES/PL/TR + Gemma 270M (with safe fallbacks) + style control\n",
        "\n",
        "!pip -q install sentence-transformers faiss-cpu transformers pandas\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
      ],
      "metadata": {
        "id": "uqTqcLapU-nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 1) Tiny multilingual dataset\n",
        "# -----------------------------\n",
        "rows = [\n",
        "    # English\n",
        "    {\"lang\":\"en\",\"phrase\":\"spill the tea\",\"meaning\":\"share gossip or a secret\",\"usage\":\"She spilled the tea about the new launch.\",\"source_url\":\"https://example.com\"},\n",
        "    {\"lang\":\"en\",\"phrase\":\"low-key\",\"meaning\":\"subtly; a little bit; not openly\",\"usage\":\"I’m low-key excited about this collab.\",\"source_url\":\"https://example.com\"},\n",
        "    # Spanish\n",
        "    {\"lang\":\"es\",\"phrase\":\"estar en las nubes\",\"meaning\":\"estar distraído; no prestar atención\",\"usage\":\"En clase siempre está en las nubes.\",\"source_url\":\"https://example.com\"},\n",
        "    {\"lang\":\"es\",\"phrase\":\"ponerse las pilas\",\"meaning\":\"empezar a esforzarse; ponerse activo\",\"usage\":\"Tenemos que ponernos las pilas antes del lanzamiento.\",\"source_url\":\"https://example.com\"},\n",
        "    # Polish\n",
        "    {\"lang\":\"pl\",\"phrase\":\"mieć muchy w nosie\",\"meaning\":\"być markotnym; mieć zły humor\",\"usage\":\"Dziś ma muchy w nosie i nie chce rozmawiać.\",\"source_url\":\"https://example.com\"},\n",
        "    {\"lang\":\"pl\",\"phrase\":\"nie być w sosie\",\"meaning\":\"mieć gorszy dzień; być nie w nastroju\",\"usage\":\"Szef jest dziś nie w sosie.\",\"source_url\":\"https://example.com\"},\n",
        "    # Turkish\n",
        "    {\"lang\":\"tr\",\"phrase\":\"kafayı yemek\",\"meaning\":\"çok sinirlenmek ya da aklını kaçıracak gibi olmak\",\"usage\":\"Sunum bozulunca az daha kafayı yiyordum.\",\"source_url\":\"https://example.com\"},\n",
        "    {\"lang\":\"tr\",\"phrase\":\"gaza gelmek\",\"meaning\":\"kolayca coşup harekete geçmek; kışkırtılmak\",\"usage\":\"Arkadaşları söyleyince hemen gaza geldi.\",\"source_url\":\"https://example.com\"},\n",
        "]\n",
        "df = pd.DataFrame(rows)\n",
        "display(df)\n"
      ],
      "metadata": {
        "id": "36z4Qx8eVASD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------------------\n",
        "# 2) Multilingual embeddings + FAISS index\n",
        "# ------------------------------------------\n",
        "# Use a compact multilingual model so all 4 languages live in the same space\n",
        "EMB_ID = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "emb_model = SentenceTransformer(EMB_ID)\n",
        "\n",
        "# What we embed for retrieval (language-agnostic)\n",
        "df[\"blob\"] = df[\"phrase\"] + \" — \" + df[\"meaning\"] + \" — \" + df[\"usage\"]\n",
        "embeddings = emb_model.encode(df[\"blob\"].tolist(), normalize_embeddings=True)\n",
        "\n",
        "# Cosine via inner product on normalized vectors\n",
        "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "index.add(embeddings)\n"
      ],
      "metadata": {
        "id": "Y43JDp8zVCD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# 3) Generator: prefer Gemma 270M, fallback\n",
        "# -----------------------------------------\n",
        "# Some Gemma variants require accepting license / HF login.\n",
        "# We'll attempt Gemma-270M first; if that fails, fall back to a tiny open model.\n",
        "MODEL_CANDIDATES = [\n",
        "    \"google/gemma-3-270m\",     # preferred (if available to you)\n",
        "    \"google/gemma-2-2b-it\",    # small-ish instruct fallback\n",
        "    \"google/flan-t5-small\"     # tiny fallback that always works\n",
        "]\n",
        "\n",
        "loaded_id = None\n",
        "gen = None\n",
        "for mid in MODEL_CANDIDATES:\n",
        "    try:\n",
        "        tok = AutoTokenizer.from_pretrained(mid)\n",
        "        if \"t5\" in mid.lower():\n",
        "            # text2text style\n",
        "            gen = pipeline(\"text2text-generation\", model=mid, tokenizer=tok)\n",
        "        else:\n",
        "            # causal LM style\n",
        "            lm = AutoModelForCausalLM.from_pretrained(mid)\n",
        "            gen = pipeline(\"text-generation\", model=lm, tokenizer=tok)\n",
        "        loaded_id = mid\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Could not load {mid}: {e}\")\n",
        "\n",
        "print(f\"[info] Loaded generator: {loaded_id}\")\n"
      ],
      "metadata": {
        "id": "O8cSM34iVD5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------\n",
        "# 4) Retrieval & explanation\n",
        "# -------------------------------\n",
        "STYLES = {\n",
        "    \"learner\": \"Explain simply for language learners. Avoid slang in the explanation and include ONE short example.\",\n",
        "    \"casual\":  \"Use a casual, friendly tone and keep it short.\",\n",
        "    \"formal\":  \"Use a clear, formal, brand-safe tone suitable for documentation.\"\n",
        "}\n",
        "\n",
        "def search(query: str, k: int = 3):\n",
        "    qv = emb_model.encode([query], normalize_embeddings=True)\n",
        "    D, I = index.search(qv, k)\n",
        "    hits = df.iloc[I[0]].copy()\n",
        "    hits[\"score\"] = [float(s) for s in D[0]]\n",
        "    return hits\n",
        "\n",
        "def _gen_text(prompt: str):\n",
        "    # Works for both text2text and causal pipelines\n",
        "    if \"t5\" in (loaded_id or \"\").lower():\n",
        "        return gen(prompt, max_new_tokens=140)[0][\"generated_text\"]\n",
        "    else:\n",
        "        return gen(prompt, max_new_tokens=140, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "def explain(query: str, style: str = \"learner\", k: int = 3):\n",
        "    hits = search(query, k)\n",
        "    context = \"\\n\".join([f\"- [{r.lang}] {r.phrase}: {r.meaning} (e.g., {r.usage})\" for _, r in hits.iterrows()])\n",
        "    style_instr = STYLES.get(style, STYLES[\"learner\"])\n",
        "\n",
        "    prompt = (\n",
        "        f\"Explain the expression '{query}'. {style_instr}\\n\"\n",
        "        f\"Use the retrieved examples below as context and mention the language code in examples.\\n\"\n",
        "        f\"Retrieved examples:\\n{context}\\n\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "    out = _gen_text(prompt)\n",
        "    return hits[[\"lang\",\"phrase\",\"meaning\",\"usage\",\"source_url\",\"score\"]], out\n"
      ],
      "metadata": {
        "id": "-LdD7q7CVFxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------\n",
        "# 5) Run one example per language\n",
        "# -----------------------------------\n",
        "queries = [\"spill the tea\", \"estar en las nubes\", \"mieć muchy w nosie\", \"kafayı yemek\"]\n",
        "for q in queries:\n",
        "    table, answer = explain(q, style=\"learner\", k=3)\n",
        "    display(table)\n",
        "    print(f\"\\n--- {q} | learner ---\\n{answer}\\n\")\n",
        "\n",
        "# Show style customisation on one term\n",
        "_, ans_casual = explain(\"spill the tea\", style=\"casual\", k=3)\n",
        "print(\"\\n=== STYLE: CASUAL ===\\n\", ans_casual)\n",
        "_, ans_formal = explain(\"spill the tea\", style=\"formal\", k=3)\n",
        "print(\"\\n=== STYLE: FORMAL ===\\n\", ans_formal)\n"
      ],
      "metadata": {
        "id": "JHqxYR_rUsxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RfIhAPHzVGXy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}